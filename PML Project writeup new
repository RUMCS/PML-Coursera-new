FINAL PROJECT – PRACTICAL MACHINE LEARNING
==========================================
As a first step we download the datasets using the read.csv function. Many invalid values were detected during the data analysis and 
they were taken care of, values as “NA”, and “#DIV/0!”. We are unable of using any ML processes if data contains any of these 
aforementioned values.  

We found that 60 variables have completed data , therefore we will use these variables to build the prediction algorithm. 
From the structure of the data, we can see that the first 6 

The variables such as user_name, variables such as, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, 
num_window seem to be non-meaningful for our purpose therefore it seems that they will not be useful to predict the activity the 
subjects are performing. Therefore, we are going to leave those 6 columns out before we build the algorithm. In addition, to make the 
columns easier to deal with, we will go ahead and convert all features to numeric class.

Cross Validation
================
In this case, we will center in using two of the most used prediction algorithms namely RF and GBM.,
For this we divide the test set into two splits for CROSS VALIDATION. First we will allocate 80% of the data to the train model and 
20% to make a validation.

We will apply both models to predict, and we will choose the one with better accuracy, provided by the OOB estimation of error.

First we try the Random Forrest model, the output of the fitted model is:
                 ====================
Call:
 randomForest(formula = classe ~ ., data = trainData, method = "rf",      prox = TRUE) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 7

        OOB estimate of  error rate: 0.32%
Confusion matrix:
     A    B    C    D    E  class.error
A 4463    0    1    0    0 0.0002240143
B    8 3027    3    0    0 0.0036208032
C    0   12 2723    3    0 0.0054784514
D    0    0   17 2555    1 0.0069957248
E    0    0    1    5 2880 0.0020790021

Then we apply the Generalized Boosted Regression model.
                  =====================================
Next, we will try the Generalized Boosted Regression Models.
Stochastic Gradient Boosting 

15699 samples
   52 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 15699, 15699, 15699, 15699, 15699, 15699, ... 
Resampling results across tuning parameters:

  interaction.depth  n.trees  Accuracy   Kappa    
  1                   50      0.7504415  0.6835716
  1                  100      0.8194115  0.7714248
  1                  150      0.8524183  0.8132280
  2                   50      0.8541806  0.8152518
  2                  100      0.9065809  0.8817637
  2                  150      0.9305811  0.9121570
  3                   50      0.8954028  0.8675725
  3                  100      0.9396903  0.9236943
  3                  150      0.9588972  0.9480016

Tuning parameter 'shrinkage' was held constant at a value of 0.1
Tuning parameter 'n.minobsinnode' was held
 constant at a value of 10
Accuracy was used to select the optimal model using  the largest value.
The final values used for the model were n.trees = 150, interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode
 = 10. 


> 

Then we present the confusion matrix statistics.


Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1099   24    0    1    2
         B    8  714   18    3    9
         C    5   18  655   20    6
         D    2    0    7  613   10
         E    2    3    4    6  694

Overall Statistics
                                         
               Accuracy : 0.9623         
                 95% CI : (0.9558, 0.968)
    No Information Rate : 0.2845         
    P-Value [Acc > NIR] : < 2.2e-16      
                                         
                  Kappa : 0.9523         
 Mcnemar's Test P-Value : 0.002611       

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9848   0.9407   0.9576   0.9533   0.9626
Specificity            0.9904   0.9880   0.9849   0.9942   0.9953
Pos Pred Value         0.9760   0.9495   0.9304   0.9699   0.9788
Neg Pred Value         0.9939   0.9858   0.9910   0.9909   0.9916
Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
Detection Rate         0.2801   0.1820   0.1670   0.1563   0.1769
Detection Prevalence   0.2870   0.1917   0.1795   0.1611   0.1807
Balanced Accuracy      0.9876   0.9644   0.9712   0.9738   0.9789

# run the generalized boosted regression model
gbmFit <- train(classe~., data = trainData, method ="gbm", verbose = FALSE)
gbmFit
# use model to predict on validation data set
gbmPred <- predict(gbmFit, validation)
# predicted result
confusionMatrix(gbmPred, validation$classe)
We can see that the RF is the algorithm with the smallest error: 0.43%, OOB.
Finally, we predict the model with the test set, achieving these results:
> predict(rfFit, test)
 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 
 B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B 
